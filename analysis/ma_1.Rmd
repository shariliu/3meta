---
title: "Meta-analysis 1"
author: "Shari Liu"
date: "3/24/2021"
output: 
  html_document:
    code_folding: hide
---

```{r setup, message = FALSE}
options(scipen = 999, digits = 4)
knitr::opts_chunk$set(comment = "#", cache=TRUE)
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"

options(repos = r)

## load required packages
ipak <- function (pkg) {
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

packages <- c("tidyverse", "Hmisc", "lattice", "multcomp", "lsmeans", "schoRsch", "influence.ME", "lme4", "effects", "lmerTest", "cowplot", "irr", "simr", "plyr", "dplyr","patchwork", "wesanderson", "MuMIn", "devtools", "dplyr", "ggResidpanel", "HLMdiag", "mixed", "sjPlot", "effectsize")

ipak(packages)

# set summed contrasts
options(contrasts = c("contr.sum", "contr.poly")) 

# fix bs with dplyr 
detach("package:dplyr", unload = TRUE)
library("dplyr")

sessionInfo()
```

```{r import.data, eval=FALSE}
# uncomment and evaluate to generate data combined w study designs

# ind.data.pre <- read.csv("./ma_study_data.csv", header=TRUE) %>%
#   mutate(experiment=str_sub(MA_condition,1,1)) %>%
#   mutate_if(is.character,as.factor) %>%
#   mutate(subj = as.factor(subj),
#          condition = MA_condition)%>%
#   mutate_at(.vars="condition", .funs = tolower)
# 
# ma.data <- read.csv("./ma_study_designs.csv") %>%
#   rename(paper = study_ID,
#          condition = expt_condition,
#          experiment = expt_num,
#          task = looking_task) %>%
#     mutate_if(is.character,as.factor) %>%
#   mutate_at(.vars="condition", .funs = tolower)
# 
# study.design <- ma.data %>%
#   select(paper, experiment, condition, task, training_yesno, action_consequence, actor_hand, agent_efficient_fam, object_diff_size_huge, action_causal, action_consequence, location_object_goal_ambiguous, bothobjects_present_visible_fam, agent, PI_group) %>%
#     mutate_if(is.character,as.factor) %>%
#   mutate(experiment = as.factor(experiment)) %>%
#   filter(paper != "sommerville2005",
#          task != "causes")
# 
# ma.data %>%
#   select(paper, long_cite) %>%
#   unique() %>%
#   knitr::kable()
# # merge study design with individual looks from babies
# 
# ind.data <- left_join(ind.data.pre, study.design,
#                       by=c("paper", "experiment", "condition")) %>%
#   mutate(sex = tolower(str_sub(sex,1,1)),
#          look_pref = unexp_look - exp_look) %>%
#   mutate(paper = str_replace_all(paper, "unpublished", "unpub")) %>%
#   mutate(task = str_replace_all(task, "efficiency", "constraints")) %>%
#   mutate_if(is.character,as.factor)
# 
# str(ind.data)
# write.csv(ind.data, "fulldata.csv")
# View(ind.data)

```

```{r helper.functions}
# function that returns column of standardized betas from lmer model
gen.beta <- function(model) {
    df <- data.frame(fixef(model))
    names(df) <- c("beta")
    return(df)
}

# function that computes CIs and returns them in df
gen.ci <- function(model) {
  df <- data.frame(confint(model))
  names(df) <- c("lower", "upper")
  return(df)
}

# function that converts model summary (lmer) to df
gen.m <- function(model) {
  df <- data.frame(coef(summary(model)))
  names(df) <- c("b", "se", "df", "t", "p")
  return(df)
}

# function that converts model summary (lm) to df
gen.lm <- function(model) {
  df <- data.frame(coef(summary(model)))
  names(df) <- c("b", "se", "t", "p")
  return(df)
}

# function that returns age info and number of females in a dataset
ages <- function(longdata) {
  longdata %>% summarize(mean = mean(ageday), min=range(ageday)[1], max=range(ageday)[2], f=sum(sex=="f")/2)
}

# function that returns formatted result from lme4/lmerTest table
report <- function(table, index, places, tails, flip) {
  if (tails == "1") {
    p <- round(table$p[index], places)/2
    howmanytails <- "one-tailed"
  } else {
    p <- round(table$p[index], places)
    howmanytails <- "two-tailed"
  }
  if (p < .001) {
    p <- "<.001"
  } else {
    p <- paste("=", round(p, places), sep = "")
  }
  if (missing(flip)) {
    result <- paste("[", round(table$lower[index], places), ",", round(table$upper[index], places), "], ß=", round(table$beta[index], places), ", B=", round(table$b[index],places), ", SE=", round(table$se[index],places), ", p", p, ", ", howmanytails, sep = "")
  } else {
    result <- paste("[", -round(table$upper[index], places), ",", -round(table$lower[index], places), "], ß=", -round(table$beta[index], places), ", B=", -round(table$b[index],places), ", SE=", round(table$se[index],places), ", p", p, ", ", howmanytails, sep = "")
  }
  return(result)
}

se <- function(x, na.rm = FALSE) {
 if (na.rm) x <- na.omit(x)
 return(sd(x) / sqrt(length(x)))
}

describe <- function(dataset){
  summary <- dataset %>%
  summarise(lookdiff_avg = mean(look_pref, na.rm=TRUE),
            lookdiff_SE = se(look_pref,na.rm=TRUE),
            n = n())
  paste(
        #"M_unexp = ", summary$unexp_avg, "s ",
        #"M_exp = ", summary$exp_avg, "s ",
        "Mean looking preference = ", round(summary$lookdiff_avg,3), "seconds, ",
        "standard error (SE) = ", round(summary$lookdiff_SE,3)
  )
}
```

```{r within_variance}
## Retrieved from : http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/#error-bars-for-within-subjects-variables
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
  library(plyr)
  
  # New version of length which can handle NA's: if na.rm==T, don't count them
  length2 <- function (x, na.rm=FALSE) {
    if (na.rm) sum(!is.na(x))
    else       length(x)
  }
  
  # This does the summary. For each group's data frame, return a vector with
  # N, mean, and sd
  datac <- ddply(data, groupvars, .drop=.drop,
                 .fun = function(xx, col) {
                   c(N    = length2(xx[[col]], na.rm=na.rm),
                     mean = mean   (xx[[col]], na.rm=na.rm),
                     sd   = sd     (xx[[col]], na.rm=na.rm)
                   )
                 },
                 measurevar
  )
  
  # Rename the "mean" column    
  datac <- plyr::rename(datac, c("mean" = measurevar))
  
  datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean
  
  # Confidence interval multiplier for standard error
  # Calculate t-statistic for confidence interval: 
  # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
  ciMult <- qt(conf.interval/2 + .5, datac$N-1)
  datac$ci <- datac$se * ciMult
  
  return(datac)
}
## Norms the data within specified groups in a data frame; it normalizes each
## subject (identified by idvar) so that they have the same mean, within each group
## specified by betweenvars.
##   data: a data frame.
##   idvar: the name of a column that identifies each subject (or matched subjects)
##   measurevar: the name of a column that contains the variable to be summariezed
##   betweenvars: a vector containing names of columns that are between-subjects variables
##   na.rm: a boolean that indicates whether to ignore NA's
normDataWithin <- function(data=NULL, idvar, measurevar, betweenvars=NULL,
                           na.rm=FALSE, .drop=TRUE) {
  library(plyr)
  
  # Measure var on left, idvar + between vars on right of formula.
  data.subjMean <- ddply(data, c(idvar, betweenvars), .drop=.drop,
                         .fun = function(xx, col, na.rm) {
                           c(subjMean = mean(xx[,col], na.rm=na.rm))
                         },
                         measurevar,
                         na.rm
  )
  
  # Put the subject means with original data
  data <- merge(data, data.subjMean)
  
  # Get the normalized data in a new column
  measureNormedVar <- paste(measurevar, "_norm", sep="")
  data[,measureNormedVar] <- data[,measurevar] - data[,"subjMean"] +
    mean(data[,measurevar], na.rm=na.rm)
  
  # Remove this subject mean column
  data$subjMean <- NULL
  
  return(data)
}

## Summarizes data, handling within-subjects variables by removing inter-subject variability.
## It will still work if there are no within-S variables.
## Gives count, un-normed mean, normed mean (with same between-group mean),
##   standard deviation, standard error of the mean, and confidence interval.
## If there are within-subject variables, calculate adjusted values using method from Morey (2008).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   betweenvars: a vector containing names of columns that are between-subjects variables
##   withinvars: a vector containing names of columns that are within-subjects variables
##   idvar: the name of a column that identifies each subject (or matched subjects)
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySEwithin <- function(data=NULL, measurevar, betweenvars=NULL, withinvars=NULL,
                            idvar=NULL, na.rm=FALSE, conf.interval=.95, .drop=TRUE) {
  
  # Ensure that the betweenvars and withinvars are factors
  factorvars <- vapply(data[, c(betweenvars, withinvars), drop=FALSE],
                       FUN=is.factor, FUN.VALUE=logical(1))
  
  if (!all(factorvars)) {
    nonfactorvars <- names(factorvars)[!factorvars]
    message("Automatically converting the following non-factors to factors: ",
            paste(nonfactorvars, collapse = ", "))
    data[nonfactorvars] <- lapply(data[nonfactorvars], factor)
  }
  
  # Get the means from the un-normed data
  datac <- summarySE(data, measurevar, groupvars=c(betweenvars, withinvars),
                     na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)
  
  # Drop all the unused columns (these will be calculated with normed data)
  datac$sd <- NULL
  datac$se <- NULL
  datac$ci <- NULL
  
  # Norm each subject's data
  ndata <- normDataWithin(data, idvar, measurevar, betweenvars, na.rm, .drop=.drop)
  
  # This is the name of the new column
  measurevar_n <- paste(measurevar, "_norm", sep="")
  
  # Collapse the normed data - now we can treat between and within vars the same
  ndatac <- summarySE(ndata, measurevar_n, groupvars=c(betweenvars, withinvars),
                      na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)
  
  # Apply correction from Morey (2008) to the standard error and confidence interval
  #  Get the product of the number of conditions of within-S variables
  nWithinGroups    <- prod(vapply(ndatac[,withinvars, drop=FALSE], FUN=nlevels,
                                  FUN.VALUE=numeric(1)))
  correctionFactor <- sqrt( nWithinGroups / (nWithinGroups-1) )
  
  # Apply the correction factor
  ndatac$sd <- ndatac$sd * correctionFactor
  ndatac$se <- ndatac$se * correctionFactor
  ndatac$ci <- ndatac$ci * correctionFactor
  
  # Combine the un-normed means with the normed results
  merge(datac, ndatac)
}
```

```{r summary}


ind.data <- read.csv("./fulldata.csv", header=TRUE)
str(ind.data)

ind.data.long <- ind.data %>%
         rename(expected = exp_look,
                unexpected = unexp_look) %>%
         pivot_longer(cols = c(expected, unexpected),
                      names_to = "trial_type",
                      values_to = "looking_time")

ind.data.summary <- summarySEwithin(data = ind.data.long, measurevar = "looking_time", betweenvars = c("task", "paper", "experiment", "condition"), withinvars = "trial_type")

ind.pref.summary <- summarySE(data = ind.data, measurevar = "look_pref", groupvars = c("task", "paper", "experiment", "condition"))

summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) 
  
n_infants <- ind.data.summary %>%
  filter(trial_type == "expected") %>% # just take one row per study
  summarise(sum(N))

n_conditions <- ind.data.summary %>%
  filter(trial_type == "expected") %>% # just take one row per study
  nrow()

n_papers <- ind.data.summary %>%
  filter(trial_type == "expected") %>% # just take one row per study
  group_by(paper) %>%
  count(paper) %>%
  nrow()

```

# Summary of data sources
We searched for all journal papers, theses, and conference proceedings that reported looking time data from typically developing infants between 2 and 4 months of age in a task structured like the goals or constraints task. We also emailed two listservs (Cognitive Development Society, and Infant Studies) to request more datasets. This resulted in a final list of 11 papers and 35 conditions. We contacted the authors and asked them to send us the original datasets from this past published and unpublished work. We were able to gather original datasets from `r n_papers` papers, `r n_conditions` conditions, and `r length(unique(ind.data$subj))` infants (age range `r floor(min(ind.data$ageday))`-`r floor(max(ind.data$ageday))`. A team of researchers were then randomly assigned to look through relevant papers including supplemental materials (with SL double-checking every entry and resolving disagreements). When exact values were not provided, or when we came across other ambiguities (e.g. numbers reported in the paper differ from the numbers calculated using the raw data), the team contacted authors to try and address, and also used the tool WebplotDigitizer (https://automeris.io/WebPlotDigitizer/) to extract estimated values from figures. If there were discrepancies between the paper, figures and/or raw data, we prioritized the values from the raw data if available, then author correspondence, then paper, then estimates from figures, in that order. For individual datasets, authors were asked to provide their data and a codebook, and were asked for permission to share their stimuli and data publicly on OSF. Of `r n_papers` papers (`r n_conditions`, conditions), data from xx conditions and stimuli from xx conditions (either actual study videos, or example stimuli) are publicly available at https://osf.io/zwncg/.

```{r table.experiments}
table.goals <- ind.data %>%
  filter(task=="goals") %>%
  group_by(paper, condition, training_yesno, action_causal, action_consequence, location_object_goal_ambiguous, bothobjects_present_visible_fam, agent) %>%
  summarise(n = n(), agemin = floor(range(ageday)[1]), agemax = floor(range(ageday)[2])) %>%
  select(paper, condition, n, agemin, agemax, training_yesno, action_causal, action_consequence, location_object_goal_ambiguous, bothobjects_present_visible_fam, agent)

table.constraints <- ind.data %>%
  filter(task=="constraints") %>%
  group_by(paper, condition, training_yesno, action_causal, action_consequence, agent_efficient_fam, actor_hand) %>%
  summarise(n = n(), agemin = floor(range(ageday)[1]), agemax = floor(range(ageday)[2])) %>%
  select(paper, condition, n, agemin, agemax, training_yesno, action_causal, action_consequence, agent_efficient_fam, actor_hand)


knitr::kable(table.goals)

knitr::kable(table.constraints)
```

```{r normal.dist.check}
str(ind.data)

# data appear to be normally distributed
dist1 <- ggplot(ind.data, aes(x=look_pref)) +
  geom_histogram()+
  theme_cowplot(12)+
  xlab("")

dist2 <- ggplot(ind.data, aes(x=look_pref, fill=paper)) +
  geom_histogram()+
  facet_wrap(~task+paper,nrow=3)+
  theme_cowplot(12)+
  xlab("Looking time Preference in seconds (unexpected - expected)")

(dist1 | dist2)  + plot_layout(widths = c(1,4)) + plot_annotation(tag_levels = 'A')
```
Data appear to be normally distributed, no need to log transform.

# Plots

Looking to the expected vs unexpected event for all individual conditions.
```{r plots.looks}

plot1 <- ggplot(ind.data.long,
       aes(trial_type, looking_time, fill=paper))+
  theme_cowplot(10)+
  theme(legend.title = element_blank())+
  geom_boxplot(aes(alpha=str_match(trial_type, "unexpected|expected")[,1]))+
  geom_point(alpha=0.2)+
  geom_line(aes(group=subj), alpha=0.2)+
  stat_summary(geom="point", fun="mean", colour="black")+
  # geom_errorbar(data = ind.data.summary, colour="red", position = position_dodge(width = 5), width = 0, aes(ymin=looking_time-se, ymax=looking_time+se)) +
  stat_summary(geom="errorbar", fun.data="mean_se", width=0.2, colour="black")+
  ylab("Looking Time (s)")+
  xlab("Condition")+
  facet_wrap(~task+paper+condition, nrow=5, labeller=label_wrap_gen(width=10, multi_line=TRUE))+
  theme(axis.text.x = element_text(angle = 90, hjust=0.95, vjust =0.2),
        legend.position="bottom")+
  scale_alpha_discrete(range = c(0.3, 1))

plot1
```


```{r}

expunexp <- ggplot(ind.data, aes(x = log(exp_look), y=log(unexp_look))) +
    geom_point() +
  geom_smooth(method = "glm")+
  xlab("Looking to expected test events (log sec)")+
  ylab("Looking to unexpected test events (log sec)")+
  facet_wrap(~paper, nrow=2)+
  theme_cowplot(10)+
  background_grid()
```


Looking preference (unexpected - expected) for all individual conditions.
```{r plots.lookdiffs}

plot2.goals <- ggplot(ind.data %>% filter(task == "goals"),
                 aes(condition, look_pref, fill=paper)) +
  geom_boxplot(outlier.alpha = 0.2, position = position_dodge2(preserve = "single"))+
  geom_hline(yintercept = 0)+
  geom_point(alpha=0.2)+
  stat_summary(geom="point", fun="mean", colour="black")+
  stat_summary(geom="errorbar", fun.data="mean_se", width=0.2, colour="black")+
  theme_cowplot(10)+
  facet_grid(~paper, scales = "free_x",space = "free", labeller = label_wrap_gen())+
  ylab("Looking preference (s) \n <- Expected ------- Unexpected ->")+
  xlab("Condition")+
  coord_cartesian(ylim=c(-40,40))+
  theme(axis.text.x = element_text(angle = 90, hjust=0.95, vjust =0.2), legend.position = "none")+
  scale_fill_brewer(palette = "Dark2")

plot2.constraints <- ggplot(ind.data %>% filter(task == "constraints"),
                 aes(condition, look_pref, fill=paper)) +
  geom_boxplot(outlier.alpha = 0.2, position = position_dodge2(preserve = "single"))+
  geom_hline(yintercept = 0)+
  geom_point(alpha=0.2)+
  stat_summary(geom="point", fun="mean", colour="black")+
  stat_summary(geom="errorbar", fun.data="mean_se", width=0.2, colour="black")+
  theme_cowplot(10)+
  facet_grid(~paper,scales = "free_x",space = "free", labeller = label_wrap_gen())+
  ylab("")+
  xlab("")+
  coord_cartesian(ylim=c(-40,40))+
  theme(axis.text.x = element_text(angle = 90, hjust=0.95, vjust =0.2), legend.position = "none")+
  scale_fill_manual(values = wes_palette("Royal2"))

bottomrow <- (plot2.goals | plot2.constraints) + plot_layout(widths = c(1.4, 1)) 
bottomrow
expunexp / bottomrow + plot_annotation(tag_levels = 'A')

```

Looking preference (unexpected - expected) vs age for all individual conditions.

```{r plot.ageeffects, fig.width=8}

(plot3 <- ggplot(ind.data,
                 aes(ageday, look_pref, colour=paper)) +
  # geom_boxplot(outlier.alpha = 0.2, position = position_dodge2(preserve = "single"))+
  geom_hline(yintercept = 0)+
  geom_point()+
  geom_smooth(method="lm")+
  # stat_summary(geom="point", fun="mean", colour="black")+
  # stat_summary(geom="errorbar", fun.data="mean_se", width=0.2, colour="black")+
  theme_cowplot(12)+
  facet_grid(~task+paper)+
  ylab("Looking preference (s) \n <- Expected ------- Unexpected ->")+
  theme(axis.text.x = element_text(angle = 90, hjust=0.95, vjust =0.2)))

```

# Confirmatory Analysis
## Constraints Task
```{r}

# fix bs with dplyr 
detach("package:dplyr", unload = TRUE)
library("dplyr")

constraints <- ind.data %>% filter(task =="constraints")

# checking ref levels for all factors
constraints$actor_hand <- factor(constraints$actor_hand, levels=c("bare", "gloved", "mittened")) # for summed contr last level is dropped
constraints$action_consequence <- relevel(as.factor(constraints$action_consequence), ref = "state_change")
constraints$agent_efficient_fam <- relevel(as.factor(constraints$agent_efficient_fam), ref = "yes")
constraints$training_yesno <- relevel(as.factor(constraints$training_yesno), ref = "yes")
constraints$action_causal <- relevel(as.factor(constraints$action_causal), ref = "yes")

```


### Baseline Effect
```{r constraints.stats.baseline}
constraints.baseline.data <- constraints %>%
  filter(condition %in% c("3_notraining",
                          "1_pickupglove",
                          "2_pickupbarehand"))

constraints.b1 <- lmer(data = constraints.baseline.data,
                       formula = look_pref ~ 1 + ageday + (1|condition))
summary(constraints.b1)

resid_panel(constraints.b1, plots = "all",
                          smoother = TRUE,
                          qqbands = TRUE)

constraints.b1.table <- sjPlot::tab_model(constraints.b1,
                                           show.std	=TRUE,
                                           show.stat=TRUE,
                                           show.df=TRUE)

constraints.b1.beta <- summary(effectsize::standardize(constraints.b1))

constraints.baseline <- cbind(
  gen.beta(effectsize::standardize(constraints.b1)),
  gen.m(constraints.b1),
  gen.ci(constraints.b1)[3:4,]
) 

constraints.baseline

cooks1 <- cooks.distance(constraints.b1, group = "subj")
dotplot_diag(x = cooks1, cutoff = "internal", name = "cooks.distance",  index=constraints.baseline.data$subj) + ylab("Cook's distance") + xlab("subjID")

excluded.subs <- c("90", "553", "86") 

constraints.b1.cooks <- lmer(data = constraints.baseline.data %>%
                         filter(!subj %in% excluded.subs),
                       formula = look_pref ~ 1 + ageday + (1|condition))


summary(constraints.b1.cooks)
plot(allEffects(constraints.b1.cooks))

constraints.baseline.cooks <- cbind(
  gen.beta(effectsize::standardize(constraints.b1.cooks)),
  gen.m(constraints.b1.cooks),
  gen.ci(constraints.b1.cooks)[3:4,]
) 

```

For standard versions of the constraints task, we found no differential looking between the expected and unexpected events, `r describe(constraints.baseline.data)`, `r report(constraints.baseline,2,3,2)`. This result held when we removed 3 influential observations, identified using Cook's distance, `r report(constraints.baseline.cooks,2,3,2)`.

### Intervention Analysis
```{r analysis}

constraints.m1 <- lmer(data = constraints,
     formula = look_pref ~ training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + ageday + (1|condition) + (1|experiment) + (1|paper))

summary(constraints.m1)

resid_panel(constraints.m1, plots = "all",
                          smoother = TRUE,
                          qqbands = TRUE)

cooks1 <- cooks.distance(constraints.m1, group = "subj")
dotplot_diag(x = cooks1, cutoff = "internal", name = "cooks.distance", index=constraints$subj) + ylab("Cook's distance") + xlab("ID")


sjPlot::plot_model(constraints.m1,
                   type = "est",
                   colors = "bw",
                   sort.est=TRUE,
                   axis.title=c("Effect (Unexpected - Expected in seconds)"),
                   show.values=TRUE,
                   show.p=TRUE)


constraints.m1.table <- sjPlot:: tab_model(constraints.m1,
                                           show.std	=TRUE,
                                           show.stat=TRUE,
                                           show.df=TRUE)

constraints.m1.beta <- summary(effectsize::standardize(constraints.m1))


constraints.interventions <- cbind(
  gen.beta(effectsize::standardize(constraints.m1)),
  gen.m(constraints.m1),
  gen.ci(constraints.m1)[3:10,]
) 

constraints.cooks<- constraints[which(cooks1 <= 4/264),]

# where are the influential observations?
nrow(constraints)
nrow(constraints.cooks)

constraints.summary <- constraints %>%
  group_by(paper,condition) %>%
  summarise(n = n())

constraints.cooksn <- constraints.cooks %>%
  group_by(paper,condition) %>%
  summarise(n_cooks = n()) 

constraints.where.influential <- full_join(constraints.summary,constraints.cooksn) %>%
  mutate(n_excluded = n - n_cooks)

knitr::kable(constraints.where.influential)

constraints.m1.cooks <- lmer(data = constraints.cooks,
     formula = look_pref ~ training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + ageday + (1|condition) + (1|experiment) + (1|paper),
     REML=FALSE)
summary(constraints.m1.cooks)

constraints.m1.cooks.beta <- lmer(data = constraints.cooks,
     formula = scale(look_pref) ~ training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + scale(ageday) + (1|condition) + (1|experiment) + (1|paper),
     REML=FALSE)
summary(constraints.m1.cooks.beta)

sjPlot::plot_model(constraints.m1.cooks.beta,
                   type = "pred",
                   # colors = "bw",
                   sort.est=TRUE,
                   axis.title=c("Effect (Unexpected - Expected in seconds)"),
                   show.values=TRUE,
                   show.p=TRUE)

plot(allEffects(constraints.m1.cooks.beta))

regressionplot1<- sjPlot::plot_model(constraints.m1.cooks.beta,
                   type = "est",
                   colors = "bw",
                   sort.est=TRUE,
                   title = "",
                   axis.labels= c("Age in days",
                                  "Hand (bare)",
                                  "Hand (gloved)",
                                  "State change",
                                  "Actor efficient during habituation",
                                  "Sticky mittens training",
                                  "Action on contact"),
                   axis.title=c("Effect on looking (unexpected - expected) in standard deviations"),
                   show.values=TRUE,
                   show.p=TRUE)

constraints.interventions.cooks <- cbind(
  gen.beta(effectsize::standardize(constraints.m1.cooks)),
  gen.m(constraints.m1.cooks),
  gen.ci(constraints.m1.cooks)[3:10,]
) 

sjPlot:: tab_model(constraints.m1.cooks.beta, show.stat=TRUE)
summary(constraints.m1.cooks.beta)
```

### Generating BFs

```{r}
constraints.full.cooks <- lmer(data = constraints.cooks,
     formula = look_pref ~ training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + ageday + (1|condition) + (1|experiment) + (1|paper), REML=FALSE,control = lmerControl(optimizer="Nelder_Mead",optCtrl=list(maxfun=200000))) # REML set to false to enable model comparison via likelihood methods, change optimizer to Nelder Mead to deal with failures to converge

constraints.predictors=c("training_yesno","action_causal","action_consequence","actor_hand","agent_efficient_fam","ageday")

constraints.bf.cooks <- data.frame(constraints.predictors) %>%
  mutate(BF.cooks = NA,
         Interpretation.Cooks = NA) %>%
  rename(Fixed.Effect = constraints.predictors)

for (predictor in constraints.predictors) {
  modelform <- update(constraints.full.cooks, as.formula(paste0(". ~ . -",predictor)))
  BF <- exp((BIC(modelform) - BIC(constraints.full.cooks))/2)
  whichrow <- which(constraints.bf.cooks$Fixed.Effect == as.character(predictor))
  constraints.bf.cooks[whichrow,2] <- BF
  constraints.bf.cooks[whichrow,3] <- interpret_bf(BF)
}

constraints.bf.cooks
```

```{r}
constraints.full <- lmer(data = constraints,
     formula = look_pref ~ training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + ageday + (1|condition) + (1|experiment) + (1|paper), REML=FALSE,control = lmerControl(optimizer="Nelder_Mead",optCtrl=list(maxfun=200000))) # REML set to false to enable model comparison via likelihood methods, change optimizer to Nelder Mead to deal with failures to converge

constraints.predictors=c("training_yesno","action_causal","action_consequence","actor_hand","agent_efficient_fam","ageday")

constraints.bf.full <- data.frame(constraints.predictors) %>%
  mutate(BF = NA,
         Interpretation = NA) %>%
  rename(Fixed.Effect = constraints.predictors)

for (predictor in constraints.predictors) {
  modelform <- update(constraints.full, as.formula(paste0(". ~ . -",predictor)))
  BF <- exp((BIC(modelform) - BIC(constraints.full))/2)
  whichrow <- which(constraints.bf.full$Fixed.Effect == as.character(predictor))
  constraints.bf.full[whichrow,2] <- BF
  constraints.bf.full[whichrow,3] <- interpret_bf(BF)

}

constraints.bf.full
```

```{r}
# combine BF tables
constraints.bf <- full_join(constraints.bf.full, constraints.bf.cooks) %>%
  arrange(desc(BF))

```



When we compared the effects of different variants we found that motor training  increased infants' looking preference for the unexpected event (`r report(constraints.interventions,2,3,2)`). We also found that other interventions on the actions infants saw, including seeing an action that causes an effect on contact (`r report(constraints.interventions,3,3,2)`). We did not find an effect of age, `r report(constraints.interventions,8,3,2)`. This analysis included `r nrow(constraints)` total infants, `r nrow(constraints)-nrow(constraints.cooks)` of were classified as influential by Cook's Distance. When these influential participants were excluded from the analysis, we found qualitatively equivalent results, except that the effect of seeing an action that resulted in a state change (versus location change) crossed the threshold into significance (`r report(constraints.interventions.cooks,4,3,2)`). 

The presence or absence of a significant fixed effect in a frequentist setting does not map directly to strength of evidence for that fixed effect. Thus, we computed Bayes Factors for each of these fixed effects, which expresses the strength of evidence for the H0 (i.e. the hypothesis that the fixed effect has no predictive power in the model) versus H1 (i.e. the hypothesis that the fixed effect has predictive power in the model).  For the full model `look_pref ~ training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + ageday + (1|condition) + (1|experiment) + (1|paper)`, excluding influential observations, we iteratively refit the model excluding each fixed effect, and computed the Bayes Factor that expresses the strength of evidence for the full model vs the model excluding this predictor. We used model BICs to compute Bayes Factors using the following formula `exp((BIC(modelminusfixedeffect) - BIC(fullmodel))/2)` (Wagenmakers, 2007). The Bayes Factor associated with each fixed effect is shown in Table X. There was very strong evidence that seeing a causal action predicted infants' responses at test (BF = `r constraints.bf %>% filter(Fixed.Effect =="action_causal") %>% select(BF) %>% as.numeric()`). There was moderate evidence for the hypothesis that sticky mittens training (BF = `r constraints.bf %>% filter(Fixed.Effect =="training_yesno") %>% select(BF) %>% as.numeric()`), and seeing an efficient agent initially (BF = `r constraints.bf %>% filter(Fixed.Effect =="agent_efficient_fam") %>% select(BF) %>% as.numeric()`), predicted infants' violation-of-expectation responses at test. There was moderate for the null hypothesis (i.e. for the absence of this effect) for the predictors of seeing a state change vs pickup action (BF = `r constraints.bf %>% filter(Fixed.Effect =="action_consequence") %>% select(BF) %>% as.numeric()`), and strong evidence for the null hypothesis that seeing bare vs gloved vs mittened hands (BF = `r constraints.bf %>% filter(Fixed.Effect =="actor_hand") %>% select(BF) %>% as.numeric()`), and infant age (BF = `r constraints.bf %>% filter(Fixed.Effect =="ageday") %>% select(BF) %>% as.numeric()`) affected infants' looking preference at test.


```{r}
knitr::kable(constraints.bf)
```

## Goals Task
### Baseline Effect
```{r}
goals <- ind.data %>% filter(task =="goals")
goals$action_consequence <- relevel(as.factor(goals$action_consequence), ref = "none")
goals$agent <- factor(goals$agent, levels=c("animate", "person", "hand")) # last level gets dropped in model estimates
goals$training_yesno <- relevel(as.factor(goals$training_yesno), ref = "yes")
goals$bothobjects_present_visible_fam <- relevel(as.factor(goals$bothobjects_present_visible_fam), ref = "yes")

```


```{r}
goals.baseline.data <- goals %>%
  filter(condition %in% c("1_control",
                          "1_twoobject") &
           paper %in% c("gerson2014a",
                        "luo2011"))
  
goals.b1 <- lmer(data = goals.baseline.data,
                       formula = look_pref ~ 1 + ageday + (1|condition))
goals.b1.table <- sjPlot::tab_model(goals.b1,
                                           show.std	=TRUE,
                                           show.stat=TRUE,
                                           show.df=TRUE)

goals.b1.beta <- summary(effectsize::standardize(goals.b1))

goals.baseline <- cbind(
  gen.beta(effectsize::standardize(goals.b1)),
  gen.m(goals.b1),
  gen.ci(goals.b1)[3:4,]
) 

cooks1 <- cooks.distance(goals.b1, group = "subj")
dotplot_diag(x = cooks1, cutoff = "internal", name = "cooks.distance",  index=goals.baseline.data$subj) + ylab("Cook's distance") + xlab("subjID")

excluded.subs <- c("2", "7", "6", "11") 

goals.b1.cooks <- lmer(data = goals.baseline.data %>%
                         filter(!subj %in% excluded.subs),
                       formula = look_pref ~ 1 + ageday + (1|condition))
summary(goals.b1.cooks)

goals.baseline.cooks <- cbind(
  gen.beta(effectsize::standardize(goals.b1.cooks)),
  gen.m(goals.b1.cooks),
  gen.ci(goals.b1.cooks)[3:4,]
) 

```

For standard versions of the goals task, we found differential looking between the expected and unexpected events, `r describe(goals.baseline.data)`, `r report(goals.baseline,2,3,2)`. However, this was driven by 4 influential observations - without these 4 observations, there was no longer a reliable looking preference, `r report(goals.baseline.cooks,2,3,2)`. 

### Intervention Analysis
```{r}

goals.m1 <- lmer(data = goals,
     formula = look_pref ~ training_yesno  + action_consequence + location_object_goal_ambiguous + agent + bothobjects_present_visible_fam + ageday + (1|condition) + (1|experiment) + (1|paper),
     control = lmerControl(optimizer="Nelder_Mead"))

summary(goals.m1)
confint(goals.m1)
# originally pre-registered model was rank deficient, needed to drop 2 variables
# dropped action_causal because it is redundant with action_consequence (i.e. there are no non-causal actions that are state changes)
# dropped object_diff_size_huge also because it is almost completely redundant with agent (i.e. all studies with a full human agent also are "yes" for "object_diff_size_huge")
ranef(goals.m1)
goals$paper
# aa <- allFit(goals.m1)

summary(goals.m1)
sjPlot::plot_model(goals.m1,
                   type = "est",
                   colors = "bw",
                   sort.est=TRUE,
                   axis.title=c("Effect (Unexpected - Expected in seconds)"),
                   show.values=TRUE,
                   show.p=TRUE)

sjPlot::tab_model(goals.m1, show.stat=TRUE)
plot(allEffects(goals.m1))

goals.interventions<- cbind(
  gen.beta(effectsize::standardize(goals.m1)),
  gen.m(goals.m1),
  gen.ci(goals.m1)[5:12,]
) 


cooks.goals <- cooks.distance(goals.m1, group = "subj")
dotplot_diag(x = cooks.goals, cutoff = "internal", name = "cooks.distance",  index=goals$subj) + ylab("Cook's distance") + xlab("subjID")

goals.cooks <- goals[which(cooks.goals <= 4/386),]


# where are the influential observations?
nrow(goals)
nrow(goals.cooks)

goals.summary <- goals %>%
  group_by(paper,condition) %>%
  summarise(n = n())

goals.cooksn <- goals.cooks %>%
  group_by(paper,condition) %>%
  summarise(n_cooks = n()) 

goals.where.influential <- full_join(goals.summary, goals.cooksn) %>%
  mutate(n_excluded  = n - n_cooks)

knitr::kable(goals.where.influential)

goals.m1.cooks <- lmer(data = goals.cooks,
     formula = look_pref ~ training_yesno  + action_consequence + location_object_goal_ambiguous + agent + bothobjects_present_visible_fam + ageday + (1|condition) + (1|experiment) + (1|paper),
     control = lmerControl(optimizer="Nelder_Mead"))

goals.m1.cooks.beta <- lmer(data = goals.cooks,
     formula = scale(look_pref) ~ training_yesno  + action_consequence + location_object_goal_ambiguous + agent + bothobjects_present_visible_fam + ageday + (1|condition) + (1|experiment) + (1|paper),
     control = lmerControl(optimizer="Nelder_Mead"))
summary(goals.m1.cooks.beta)

sjPlot::plot_model(goals.m1.cooks.beta,
                   type = "pred",
                   # colors = "bw",
                   sort.est=TRUE,
                   axis.title=c("Effect (Unexpected - Expected in seconds)"),
                   show.values=TRUE,
                   show.p=TRUE)

plot(allEffects(goals.m1.cooks.beta))

regressionplot2 <- sjPlot::plot_model(goals.m1.cooks.beta,
                   type = "est",
                   colors = "bw",
                   sort.est=TRUE,
                   title = "",
                   axis.labels= c("Agent (person)",
                                  "Age in days",
                                  "Sticky mittens training",
                                  "State change",
                                  "Both objects present during fam/hab",
                                  "Agent (self-propelled object)",
                                  "Unambiguous location or object goal"),
                   axis.title=c("Effect on looking (unexpected - expected) in standard deviations"),
                   show.values=TRUE,
                   show.p=TRUE)

summary(goals.m1.cooks)
plot(allEffects(goals.m1.cooks))

summary(goals.m1.cooks.beta)


goals.interventions.cooks<- cbind(
  gen.beta(effectsize::standardize(goals.m1.cooks)),
  gen.m(goals.m1.cooks),
  gen.ci(goals.m1.cooks)[5:12,]
) 

```


```{r}
knitr::kable(goals.interventions.cooks)
```

Warning - for some reason sjPlot does not populate the below table with the correct values. Please refer to the summary of `goals.m1.cooks.beta` above...
```{r}
sjPlot::tab_model(goals.m1.cooks.beta, show.stat=TRUE)

summary(goals.m1.cooks.beta)
confint(goals.m1.cooks.beta)
```


```{r}
(regressionplot1 | regressionplot2) + plot_annotation(tag_levels = 'A')
```


### Generating BFs

```{r}
goals.full <- lmer(data = goals,
     formula = look_pref ~ training_yesno  + action_consequence + location_object_goal_ambiguous + agent + bothobjects_present_visible_fam + ageday + (1|condition) + (1|experiment) + (1|paper), REML=FALSE,
     control = lmerControl(optimizer="Nelder_Mead")) # REML set to false to enable model comparison via likelihood methods, change optimizer to Nelder Mead to deal with failures to converge

goals.predictors=c("training_yesno","action_consequence","location_object_goal_ambiguous","agent","bothobjects_present_visible_fam","ageday")

goals.bf.full <- data.frame(goals.predictors) %>%
  mutate(BF = NA,
         Interpretation = NA) %>%
  rename(Fixed.Effect = goals.predictors)

for (predictor in goals.predictors) {
  modelform <- update(goals.full, as.formula(paste0(". ~ . -",predictor)))
  BF <- exp((BIC(modelform) - BIC(goals.full))/2)
  whichrow <- which(goals.bf.full$Fixed.Effect == as.character(predictor))
  goals.bf.full[whichrow,2] <- BF
  goals.bf.full[whichrow,3] <-interpret_bf(BF)
}

goals.bf.full
```

```{r}
goals.full.cooks <- lmer(data = goals.cooks,
     formula = look_pref ~ training_yesno  + action_consequence + location_object_goal_ambiguous + agent + bothobjects_present_visible_fam + ageday + (1|condition) + (1|experiment) + (1|paper), REML=FALSE,
     control = lmerControl(optimizer="Nelder_Mead")) # REML set to false to enable model comparison via likelihood methods, change optimizer to Nelder Mead to deal with failures to converge

goals.predictors=c("training_yesno","action_consequence","location_object_goal_ambiguous","agent","bothobjects_present_visible_fam","ageday")

goals.bf.cooks <- data.frame(goals.predictors) %>%
  mutate(BF.cooks = NA,
         Interpretation.cooks = NA) %>%
  rename(Fixed.Effect = goals.predictors)

for (predictor in goals.predictors) {
  modelform <- update(goals.full.cooks, as.formula(paste0(". ~ . -",predictor)))
  BF <- exp((BIC(modelform) - BIC(goals.full.cooks))/2)
  whichrow <- which(goals.bf.cooks$Fixed.Effect == as.character(predictor))
  goals.bf.cooks[whichrow,2] <- BF
  goals.bf.cooks[whichrow,3] <- interpret_bf(BF)
}

goals.bf.cooks
```

```{r}
goals.bf <- full_join(goals.bf.full, goals.bf.cooks) %>%
    arrange(desc(BF))

knitr::kable(goals.bf)
```


We note that the analysis we reported deviates from our pre-registration plan. In that plan, we included 8 fixed effects, including a fixed effect for age in days, one picking out a control condition, and 6 others. However, 2 of these fixed effects were either completely or partially redundant with other predictors in the model, which caused a rank deficiency issue. Thus, we dropped these two fixed effects from the model. We report this new model below.  

When we compared the effects of different variants, we found that conditions that made the goal of the agent's actions unambiguous (`r report(goals.interventions,4,3,2)`), and conditions that presented two objects for the agent to choose between during habituation or familiarization (`r report(goals.interventions,7,3,2)`) reported greater infants' looking preference for the unexpected event. We did not find any other effects, including age, motor training, or other manipulations, when taking into account all of these predictors in the same model. These analyses included `r nrow(goals)` total infants, `r nrow(goals)-nrow(goals.cooks)` of whom were classified as influential based on Cook's Distance. When excluding these influential participants in the analysis, the effect of seeing an unambiguous goal held (`r report(goals.interventions.cooks,4,3,2)`), the effect of having both objects present initially was no longer significant (`r report(goals.interventions.cooks,7,3,2)`), and the effect of seeing an animate, self-propelled object as the agent became significant (`r report(goals.interventions.cooks,5,3,2)`).

We again computed Bayes Factors for each of the fixed effects, which expresses the strength of evidence for the H0 (i.e. the hypothesis that the fixed effect has no predictive power in the model) versus H1 (i.e. the hypothesis that the fixed effect has predictive power in the model). The Bayes Factor associated with each fixed effect is shown in Table X. There was anecdotal evidence that seeing an unambiguous object or location goal  (BF = `r goals.bf %>% filter(Fixed.Effect =="location_object_goal_ambiguous") %>% select(BF) %>% as.numeric()`), and seeing both objects during familiarization or habituation  (BF = `r goals.bf %>% filter(Fixed.Effect =="bothobjects_present_visible_fam") %>% select(BF) %>% as.numeric()`), influenced infants' responses at test. 

The only predictor for which there was any evidence was anecdotal evidence for the effect of seeing unambiguous evidence for someone’s goal (BF=`r goals.bf %>% filter(Fixed.Effect =="location_object_goal_ambiguous") %>% select(BF) %>% as.numeric()`).  Excluding influential subjects from this analysis led to a similar pattern of results, with evidence against the hypothesis that action training influenced infants’ looking preferences (moderate evidence including all subjects, BF=`r goals.bf %>% filter(Fixed.Effect =="training_yesno") %>% select(BF) %>% as.numeric()`; anecdotal evidence excluding influential subjects, BF=`r goals.bf %>% filter(Fixed.Effect =="training_yesno") %>% select(BF.cooks) %>% as.numeric()`). There was strong evidence against the hypothesis that looking preferences varied as a function of age (BF=`r goals.bf %>% filter(Fixed.Effect =="ageday") %>% select(BF) %>% as.numeric()`; BF=`r goals.bf %>% filter(Fixed.Effect =="ageday") %>% select(BF.cooks) %>% as.numeric()` excluding influential subjects).


