---
title: "3meta BRMS"
author: "Shari Liu"
date: "2/22/2022"
output: 
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
options(scipen = 999, digits = 4)
knitr::opts_chunk$set(comment = "#", echo = FALSE)
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"

options(repos = r)

## load required packages
ipak <- function (pkg) {
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

packages <- c("tidyverse", "lsmeans", "influence.ME", "lme4", "effects", "lmerTest", "cowplot", "irr", "simr", "patchwork", "wesanderson", "devtools", "effectsize", "mice", "rstan", "brms", "bayestestR", "rstanarm", "shinystan", "DT", "tidybayes", "distributional", "loo")
devtools::install_github("mvuorre/brmstools")

ipak(packages)
set.seed(429)

# set summed contrasts
options(contrasts = c("contr.sum", "contr.poly")) 

# fix bs with dplyr 
detach("package:dplyr", unload = TRUE)
library("dplyr")

sessionInfo()
```

```{r}
ind.data.init <- read.csv("./fulldata.csv", header=TRUE)
str(ind.data.init)
num_cols <- c("ageday", "exp_look", "unexp_look", "look_pref")
factor_cols <- setdiff(colnames(ind.data.init), num_cols)

ind.data <- ind.data.init %>%
  mutate_at(factor_cols,as.factor)
str(ind.data)

constraints <- ind.data %>% filter(task =="constraints")
goals <- ind.data %>% filter(task =="goals")
constraints <- ind.data %>% filter(task =="constraints")

# checking ref levels for all factors
constraints$actor_hand <- factor(constraints$actor_hand, levels=c("bare", "gloved", "mittened")) # for summed contr last level is dropped
constraints$action_consequence <- relevel(constraints$action_consequence, ref = "state_change")
constraints$agent_efficient_fam <- relevel(constraints$agent_efficient_fam, ref = "yes")
constraints$training_yesno <- relevel(constraints$training_yesno, ref = "yes")
constraints$action_causal <- relevel(constraints$action_causal, ref = "yes")

goals$action_consequence <- relevel(goals$action_consequence, ref = "none")
goals$agent <- factor(goals$agent, levels=c("object", "person", "hand")) # last level gets dropped in model estimates
goals$training_yesno <- relevel(goals$training_yesno, ref = "yes")
goals$bothobjects_present_visible_fam <- relevel(goals$bothobjects_present_visible_fam, ref = "yes")
```


# Exploratory Analysis
## Visualization

```{r}
ind.data.expunexp <- ind.data %>%
  gather(test_event, look_sec, exp_look:unexp_look) %>%
  mutate(look_logsec = log(look_sec))

exp.look <- ind.data.expunexp %>% filter(test_event == "exp_look") %>% select(look_sec) %>% as.vector()
unexp.look <- ind.data.expunexp %>% filter(test_event == "unexp_look") %>% select(look_sec) %>% as.vector()

hist(exp.look$look_sec)
hist(unexp.look$look_sec)
```

```{r}
ind.data.long <- ind.data %>%
  gather(look_type, look_sec, unexp_look:exp_look) %>%
  mutate(look_log = log(look_sec))

(eplot1 <- ggplot(ind.data, aes(x = log(exp_look), y=log(unexp_look))) +
    geom_point() +
  geom_smooth(method = "glm")+
  xlab("Looking to expected test events (log sec)")+
  ylab("Looking to unexpected test events (log sec)")+
  facet_wrap(paper~PI_group, nrow=2)+
  theme_cowplot(10)+
  background_grid()+
  labs(subtitle = "Looking in log seconds for expected vs unexpected test event \nbroken down by paper and lab")
)

(eplot2 <- ggplot(ind.data.long, aes(x = ageday, y=look_log)) +
    geom_point(aes(colour = paper, alpha=.5)) +
    geom_smooth(aes(colour = paper, alpha=.5), method = "glm")+
    ylab("Looking time (log sec)")+
    facet_wrap(~task+action_causal+look_type, nrow=2)+
    theme_cowplot(10)+
    background_grid()+
    labs(subtitle = "Looking in log seconds vs age, broken down by task, \ntest event, and whether actions were causal or not")
)

(eplot3 <- ggplot(ind.data.long, aes(x = ageday, y=look_log)) +
    geom_point(aes(colour = paper, alpha=.5)) +
    geom_smooth(aes(colour = paper, alpha=.5), method = "glm")+
    # geom_smooth(method = "lm")+
    ylab("Looking time (log sec)")+
    facet_wrap(~task+training_yesno+look_type, nrow=2)+
    theme_cowplot(10)+
    background_grid()+
    labs(subtitle = "Looking in log seconds vs age, broken down by task, \ntest event, and whether there was action training")
)

(eplot4 <- ggplot(ind.data.long %>%
                    filter(task == "goals"), aes(x = ageday, y=look_log)) +
    geom_point(aes(colour = paper, alpha=.5)) +
    geom_smooth(aes(colour = paper, alpha=.5), method = "glm")+
    # geom_smooth(method = "lm")+
    ylab("Looking time (log sec)")+
    facet_wrap(~location_object_goal_ambiguous+look_type, nrow=1)+
    theme_cowplot(10)+
    background_grid()+
    labs(subtitle = "Looking in log seconds vs age, in the goals task, \nbroken down by whether the goal was ambiguous")
)
```

```{r}
(eplot6 <- ggplot(ind.data.long, aes(x = look_type, y=look_log)) +
    geom_boxplot()+
    geom_point(aes(colour = paper, alpha=.5)) +
    # geom_smooth(aes(colour = paper, alpha=.5), method = "glm")+
    # geom_smooth(method = "lm")+
    ylab("Looking time (log sec)")+
    facet_wrap(~task+training_yesno, nrow=2)+
    theme_cowplot(10)+
    background_grid()+
    labs(subtitle = "Looking in log seconds vs age, broken down by task, \ntest event, and whether there was action training")
)
```


```{r}
hist(ind.data$look_pref)
```

## Various attempts to make the frequentist setting work (not successful)
```{r, eval=FALSE}

# trying to figure out whether random slopes are possible in the frequentist setting
constraints.e1 <- lmer(data = constraints,
     formula = look_pref ~  training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + scale(ageday)  + (1+experiment|paper), 
     control = lmerControl(optimizer="bobyqa"))

summary(constraints.e1)


cooks.constraints.e1 <- cooks.distance(constraints.e1, group = "subj")
dotplot_diag(x = cooks.constraints.e1, cutoff = "internal", name = "cooks.distance",  index=constraints$subj) + ylab("Cook's distance") + xlab("subjID")

constraints.e1.cooks <- constraints[which(cooks.constraints.e1 <= 4/264),]

constraints.e1.aftercooks <- lmer(data = constraints.e1.cooks,
     formula = look_pref ~  training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + scale(ageday)  + (1+experiment|paper), 
     control = lmerControl(optimizer="bobyqa"))

summary(constraints.e1.aftercooks)

# try other optimizers
constraints.e1.all <- allFit(constraints.e1)
summary(constraints.e1.all)
```

```{r, eval=FALSE}
constraints.full.e1 <- lmer(data = constraints,
     formula = look_pref ~  training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + scale(ageday) + (1+experiment|paper), REML=FALSE, lmerControl(optimizer="bobyqa")) # REML set to false to enable model comparison via likelihood methods, change optimizer to Nelder Mead to deal with failures to converge

constraints.predictors=c("training_yesno","action_causal","action_consequence","actor_hand","agent_efficient_fam","ageday")

constraints.bf.e1 <- data.frame(constraints.predictors) %>%
  mutate(BF.cooks = NA,
         Interpretation.Cooks = NA) %>%
  rename(Fixed.Effect = constraints.predictors)

for (predictor in constraints.predictors) {
  modelform <- update(constraints.full.e1, as.formula(paste0(". ~ . -",predictor)))
  BF <- exp((BIC(modelform) - BIC(constraints.full.e1))/2)
  whichrow <- which(constraints.bf.e1$Fixed.Effect == as.character(predictor))
  constraints.bf.e1[whichrow,2] <- BF
  constraints.bf.e1[whichrow,3] <- interpret_bf(BF)
}

constraints.bf.e1
```

```{r, eval=FALSE}
# trying to figure out whether random slopes are possible in the frequentist setting
goals.e1 <- lmer(data = goals,
     formula = look_pref ~ training_yesno  + action_consequence + location_object_goal_ambiguous + agent + bothobjects_present_visible_fam +  scale(ageday)  + (1+experiment|paper), 
     control = lmerControl(optimizer="Nelder_Mead"))

cooks.goals.e1 <- cooks.distance(goals.e1, group = "subj")
dotplot_diag(x = cooks.goals.e1, cutoff = "internal", name = "cooks.distance",  index=goals$subj) + ylab("Cook's distance") + xlab("subjID")

goals.e1.cooks <- goals[which(cooks.goals.e1 <= 4/386),]

goals.e1.aftercooks <- lmer(data = goals.e1.cooks,
     formula = look_pref ~ training_yesno  + action_consequence + location_object_goal_ambiguous + agent + bothobjects_present_visible_fam +  scale(ageday)  + (1+experiment|paper), 
     control = lmerControl(optimizer="Nelder_Mead"))

summary(goals.e1.aftercooks)
plot(allEffects(goals.e1.aftercooks))
```


## Attempts to original Sommerville data (on hold for now)
```{r}

# 0_pilotnotraining
# n = 16, mean age = 106.54
# UNEXP, M = 53.9, SD = 14.9
# EXP, M = 47.1, SD = 12.4

sommerville0.unexp <- rlnorm(16, meanlog=3.987, sdlog=.25)
mean(sommerville0.unexp)
sd(sommerville0.unexp)

sommerville0.exp <- rlnorm(16, meanlog=3.850, sdlog=.25)
mean(sommerville0.exp)
sd(sommerville0.exp)

# 1_watchfirst
# n = 15, mean age = 101.32
# UNEXP, M = 33.781, SD = 7.283212518
# EXP, M = 33.417, SD = 4.740388137
# t = 0.9

# 1_reachfirst
# n = 15, mean age = 104.32
# UNEXP, M = 53.992, SD = 11.10689909
# EXP, M = 30.504, SD = 4.369927511
# t = 2.6

```

## BRMS
In the primary analysis, we chose to pare down the random effects structure to just random intercepts for conditions, experiments, and papers, in order to deal with issues of convergence. Nevertheless, such an analytical choice neglects the correlated methodological decisions shared by conditions and experiments within papers, and leaves open the possibility that some of the effects we fit were driven by just a few conditions. When we explored ways of fitting frequentist models with alternative ways of specifying the random effects, and iteratively computing Bayes Factors like we did above, we never found a set of optimizers and random effects specifications that consistently converged. Thus, for an exploratory analysis we decided to refit the models from the confirmatory analysis with the full random effects structure using brms. 


```{r}
brms.test.hyp <- function(model, hyp.vector) {
  results <- hypothesis(model, hyp.vector, seed = 429, alpha=.05)
  return(results)
}

# function that returns formatted result from a brms.test.hyp df
report.brms <- function(df, index, places) {
  "Estimate = xx [xx, xx], Evidence Ratio = xx, Posterior Probability = xx"
  result <- paste("Estimate=", 
                  round(df$Estimate[index], places),
                  " [",
                  round(df$CI.Lower[index], places),
                  ", ",
                  round(df$CI.Upper[index], places),
                  "], Evidence Ratio=",
                  round(df$Evid.Ratio[index], places),
                  ", Posterior Probability=",
                  round(df$Post.Prob[index], places),
                  sep = "")
  return(result)
}
```




### Goals Task
```{r}
#  not run, uncomment to reproduce results
# goals.brms.intercept <- brm(formula = look_pref ~ 1 + ageday + (1|condition) + (1+condition|experiment) + (1+condition|paper),
#                 data= goals,
#                 warmup = 1000,
#                 iter   = 5000,
#                 family = student(link="identity"),
#                 chains = 6,
#                 thin = 2,
#                 seed = 429,
#                 save_all_pars = TRUE,
#                 sample_prior = TRUE,
#                 control = list(adapt_delta = .98))
# saveRDS(goals.brms.intercept,file="goals.brms.intercept.rds")
goals.brms.intercept <- readRDS("goals.brms.intercept.rds")
# shinystan::launch_shinystan(goals.brms.intercept)
summary(goals.brms.intercept)
hypothesis(goals.brms.intercept, "Intercept > 0", alpha=.05)
pp_check(goals.brms.intercept, nsamples=100)
goals.brms.intercept.diag <- loo(goals.brms.intercept)
plot(goals.brms.intercept.diag)

goals_hypothesis_vector <- c("ageday < 0",
                       "training_yesno1 > 0",
                       "location_object_goal_ambiguous1 > 0",
                       "action_consequence1 > 0",
                       "agent1 > 0",
                       "agent2 > 0", 
                       "bothobjects_present_visible_fam1 > 0"
            )

# goals.brms.full <- brm(formula = look_pref ~ training_yesno + agent + action_consequence + bothobjects_present_visible_fam + location_object_goal_ambiguous + ageday + (1|condition) + (1+condition|experiment) + (1+experiment|paper),
#                 data= goals,
#                 warmup = 1000,
#                 iter   = 5000,
#                 family = student(link="identity"),
#                 chains = 6,
#                 thin = 2,
#                 seed = 429,
#                 save_all_pars = TRUE,
#                 sample_prior = TRUE,
#                 control = list(adapt_delta = .98))
# saveRDS(goals.brms.full,file="goals.brms.full.rds")
goals.brms.full <- readRDS("goals.brms.full.rds")
# shinystan::launch_shinystan(goals.brms.full)
summary(goals.brms.full)
pp_check(goals.brms.full, nsamples=100)
goals.brms.full.diag <- loo(goals.brms.full)
plot(goals.brms.full.diag)


goals.brms.full.results <- brms.test.hyp(goals.brms.full, goals_hypothesis_vector)


# goals.brms.full.simplefx  <- brm(formula = look_pref ~ training_yesno + agent + action_consequence + bothobjects_present_visible_fam + location_object_goal_ambiguous + ageday + (1|condition) + (1|experiment) + (1|paper),
#                 data= goals,
#                 warmup = 1000,
#                 iter   = 5000,
#                 family = student(link="identity"),
#                 chains = 6,
#                 thin = 2,
#                 seed = 429,
#                 save_all_pars = TRUE,
#                 sample_prior = TRUE,
#                 control = list(adapt_delta = .98))
# saveRDS(goals.brms.full.simplefx,file="goals.brms.full.simplefx.rds")
goals.brms.full.simplerfx <- readRDS("goals.brms.full.simplefx.rds")
# shinystan::launch_shinystan(goals.brms.full.simplerfx)
summary(goals.brms.full.simplerfx)
pp_check(goals.brms.full.simplerfx, nsamples=100)
goals.brms.full.simplerfx.diag <- loo(goals.brms.full.simplerfx)
plot(goals.brms.full.simplerfx.diag)

goals.brms.full.simplerfx.results <- brms.test.hyp(goals.brms.full.simplerfx, goals_hypothesis_vector)

(goals.compare.rfx <- bayes_factor(goals.brms.full.simplerfx, goals.brms.full))
```


```{r}
knitr::kable(goals.brms.full.results[[1]])
knitr::kable(goals.brms.full.simplerfx.results[[1]])
```

The intercept only model (`look_pref ~ 1 + ageday + (1|condition) + (1+condition|experiment) + (1+condition|paper)`) provided strong evidence for looking preference larger than 0 (evidence ratio = 14.01). The full model with complex random effects (look_pref ~ training_yesno  + action_consequence + location_object_goal_ambiguous + agent + bothobjects_present_visible_fam + ageday + (1|condition) + (1+condition|experiment) + (1+experiment|paper))  yielded some similar results to the frequentist model, but also some differences: of all the predictors, seeing unambiguous evidence for what someone’s goal was had high predictive power (evidence ratio = 4.87), as did seeing both objects present during familiarization (evidence ratio = 10.46), and seeing a self-propelled object as the agent (6.29). We also found evidence for a negative age effect, with younger babies showing stronger looking preferences than older babies (11.16). Again, these BFs were substantially smaller than the full Bayesian model with simpler random effects, and all confidence intervals included 0, suggesting that substantial variance in the data is accounted for by shared methodological considerations within experiments and papers. As in the constraints task, the Bayes Factor between the full models with identical fixed effects and varying (simple vs complex) random effects strongly favored the simple random effects structure (BF = 34.44220). These results suggest that, like in the constraints task, although adding random slopes for conditions and experiments accounted for variance in infants’ looking behavior, the simple random effects structure represented in the frequentist analysis better accounted for this data. In the Bayesian version of this model, the only effects with posterior probabilities greater than 0.95 were seeing unambiguous evidence for someone’s goal, seeing a self-propelled object as an agent, and seeing both objects during familiarization or habituation, which largely agree with the findings from the frequentist analysis. (We note, however, that some of the predictors identified by this analysis appeared to be driven by just a few influential subjects, according to the frequentist analysis, but no observations reached the threshold of k>0.7 in the PSIS diagnostic plots.)


### Constraints Task
```{r}
hist(constraints$look_pref)

# #  not run, uncomment to reproduce results
# constraints.brms.intercept <- brm(formula = look_pref ~ 1 + ageday + (1|condition) + (1+condition|experiment) + (1+condition|paper),
#                 data= constraints,
#                 warmup = 1000,
#                 iter   = 5000,
#                 family = student(link="identity"),
#                 chains = 6,
#                 thin = 2,
#                 seed = 429,
#                 save_all_pars = TRUE,
#                 sample_prior = TRUE,
#                 control = list(adapt_delta = .98))
# saveRDS(constraints.brms.intercept,file="constraints.brms.intercept.rds")

constraints.brms.intercept <- readRDS("constraints.brms.intercept.rds")
# shinystan::launch_shinystan(constraints.brms.intercept)
summary(constraints.brms.intercept)
pp_check(constraints.brms.intercept, ndraws=100)
constraints.brms.intercept.diag <- loo(constraints.brms.intercept)
plot(constraints.brms.intercept.diag)

hypothesis(constraints.brms.intercept, "Intercept > 0", alpha=.05)

constraints_hypothesis_vector <- c("ageday > 0",
                       "training_yesno1 > 0",
                       "action_causal1 > 0",
                       "action_consequence1 > 0",
                       "actor_hand1 > 0",
                       "actor_hand2 > 0", 
                       "agent_efficient_fam1 > 0"
            )

# constraints.brms.full <- brm(formula = look_pref ~ training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + ageday + (1|condition) + (1+condition|experiment) + (1|paper),
#                 data= constraints,
#                 warmup = 1000,
#                 iter   = 5000,
#                 family = student(link="identity"),
#                 chains = 6,
#                 thin = 2,
#                 seed = 429,
#                 save_all_pars = TRUE,
#                 sample_prior = TRUE,
#                 control = list(adapt_delta = .98))
# saveRDS(constraints.brms.full,file="constraints.brms.full.rds")
constraints.brms.full <- readRDS("constraints.brms.full.rds")
pp_check(constraints.brms.full, ndraws=100)
# shinystan::launch_shinystan(constraints.brms.full)
summary(constraints.brms.full)
constraints.brms.full.diag <- loo(constraints.brms.full)
plot(constraints.brms.full.diag)

# constraints.brms.full.simplerfx <- brm(formula = look_pref ~ training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + ageday + (1|condition) + (1|experiment) + (1|paper),
#                 data= constraints,
#                 warmup = 1000,
#                 iter   = 5000,
#                 family = student(link="identity"),
#                 chains = 6,
#                 thin = 2,
#                 seed = 429,
#                 save_all_pars = TRUE,
#                 sample_prior = TRUE,
#                 control = list(adapt_delta = .98))
# saveRDS(constraints.brms.full.simplerfx,file="constraints.brms.full.simplerfx.rds")
constraints.brms.full.simplerfx <- readRDS("constraints.brms.full.simplerfx.rds")
# shinystan::launch_shinystan(constraints.brms.full.simplerfx)
constraints.brms.full.simplerfx.diag <- loo(constraints.brms.full.simplerfx)
plot(constraints.brms.full.simplerfx.diag)
pp_check(constraints.brms.full.simplerfx, ndraws=100)
summary(constraints.brms.full.simplerfx)


constraints.brms.full.simplerfx.results <- brms.test.hyp(constraints.brms.full.simplerfx, constraints_hypothesis_vector)

constraints.brms.full.results <- brms.test.hyp(constraints.brms.full, constraints_hypothesis_vector)

(constraints.compare.rfx <- bayes_factor(constraints.brms.full.simplerfx, constraints.brms.full))



```

```{r}
knitr::kable(constraints.brms.full.simplerfx.results[[1]])
knitr::kable(constraints.brms.full.results[[1]])
```

The intercept only model (look_pref ~ 1 + ageday + (1|condition) + (1+condition|experiment) + (1+condition|paper)) provided anecdotal against the hypothesis that there is a looking preference greater than 0 (evidence ratio = 0.44).  The full model with complex random effects (look_pref ~ training_yesno + action_causal + action_consequence + actor_hand + agent_efficient_fam + ageday + (1|condition) + (1+condition|experiment) + (1+experiment|paper)) yielded qualitatively similar results to the frequentist model (see Table x): of all the predictors, seeing an action that caused an observable outcome (evidence ratio = 3.74) had the highest predictive power on infants’ looking behavior, followed by age in days (3.05) and sticky mittens training (2.79). However, the lower confidence interval over these estimates crossed 0, and the size of the evidence ratios were substantially smaller than the Bayes Factors from the frequentist model, and smaller than the evidence ratios from the full Bayesian model with simpler random effects (see Table x), suggesting that substantial variance in the data is accounted for by shared methodological decisions within experiments and papers. (Note that the 26 conditions from this task came from just 2 papers). Nevertheless, the Bayes Factor between these two full models with identical fixed effects and varying (simple vs complex) random effects substantially favored the simple random effects structure (BF = 1998079.74). Altogether, these results suggest that although adding random slopes for conditions and experiments accounted for variance in infants’ looking behavior, simple random effects structure presented in the frequentist analysis better accounts for the data. (One caveat to this interpretation is that the number of conditions and papers were limited - only 2 papers with 7 and 5 conditions each.) In the Bayesian version of this model, the only effects with posterior probabilities greater than 0.95 were sticky mittens training, causal action, and seeing a constraint agent during habituation, which largely agree with the findings from the frequentist analysis.
